{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "059a2828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 23 08:40:39 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     Off |   00000000:48:00.0 Off |                    0 |\n",
      "|  0%   28C    P8             21W /  250W |       1MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for table 3 (generalizing to unseen small molecule perturbations - cell expression as cell line context, fingerprint for pert context)\n",
    "\n",
    "import torch\n",
    "import lightning as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "# from contextualized.easy import ContextualizedCorrelationNetworks\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "import warnings\n",
    "\n",
    "from contextualized.regression.lightning_modules import ContextualizedCorrelation\n",
    "from contextualized.data import CorrelationDataModule\n",
    "from lightning import seed_everything, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Configuration and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 242514 samples with valid SMILES...\n",
      "Found 12976 unique perturbations (SMILES)\n",
      "Perturbation split: 10380 train, 2596 test perturbations\n",
      "Sample split: 195733 train, 46781 test samples\n"
     ]
    }
   ],
   "source": [
    "PATH_L1000   = 'data/trt_cp_smiles.csv' #file filtered with only the trt_cp perts with smiles\n",
    "PATH_CTLS    = 'data/ctrls.csv'     \n",
    "N_DATA_PCS   = 50   \n",
    "PERTURBATION_HOLDOUT_SIZE = 0.2  \n",
    "RANDOM_STATE = 42\n",
    "SUBSAMPLE_FRACTION = None  # None for using full data, or decimal for percent subsample\n",
    "\n",
    "morgan_gen = rdFingerprintGenerator.GetMorganGenerator(radius=3, fpSize=4096)  \n",
    "\n",
    "# Function to generate Morgan fingerprints from SMILES\n",
    "def smiles_to_morgan_fp(smiles, generator=morgan_gen):\n",
    "    \"\"\"\n",
    "    Convert SMILES string to Morgan fingerprint using MorganGenerator.\n",
    "    \n",
    "    Args:\n",
    "        smiles (str): SMILES string\n",
    "        generator: RDKit MorganGenerator instance\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Binary fingerprint array, or array of zeros if invalid SMILES\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            warnings.warn(f\"Invalid SMILES: {smiles}\")\n",
    "            return np.zeros(generator.GetOptions().fpSize)\n",
    "        \n",
    "        fp = generator.GetFingerprint(mol)\n",
    "        return np.array(fp)\n",
    "        # return np.zeros(generator.GetOptions().fpSize)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Error processing SMILES {smiles}: {str(e)}\")\n",
    "        return np.zeros(generator.GetOptions().fpSize)\n",
    "\n",
    "#load data\n",
    "df = pd.read_csv(PATH_L1000, engine='pyarrow')\n",
    "\n",
    "# pick the perturbation to fit model on here\n",
    "pert_to_fit_on = ['trt_cp']\n",
    "df = df[df['pert_type'].isin(pert_to_fit_on)]\n",
    "\n",
    "# quality filters\n",
    "bad = (\n",
    "    (df['distil_cc_q75'] < 0.2) | (df['distil_cc_q75'] == -666) | (df['distil_cc_q75'].isna()) |\n",
    "    (df['pct_self_rank_q25'] > 5) | (df['pct_self_rank_q25'] == -666) | (df['pct_self_rank_q25'].isna())\n",
    ")\n",
    "df = df[~bad]\n",
    "\n",
    "# Filter out samples with missing SMILES\n",
    "df = df.dropna(subset=['canonical_smiles'])\n",
    "df = df[df['canonical_smiles'] != '']\n",
    "\n",
    "print(f\"Processing {len(df)} samples with valid SMILES...\")\n",
    "\n",
    "if SUBSAMPLE_FRACTION is not None:\n",
    "    df = df.sample(frac=SUBSAMPLE_FRACTION, random_state=RANDOM_STATE)\n",
    "    print(f\"Subsampled to {len(df)} samples ({SUBSAMPLE_FRACTION*100}% of data)\")\n",
    "\n",
    "# PERTURBATION HOLDOUT: Split unique perturbations first\n",
    "unique_smiles = df['canonical_smiles'].unique()\n",
    "print(f\"Found {len(unique_smiles)} unique perturbations (SMILES)\")\n",
    "\n",
    "# Split unique SMILES into train and test sets\n",
    "smiles_train, smiles_test = train_test_split(\n",
    "    unique_smiles, \n",
    "    test_size=PERTURBATION_HOLDOUT_SIZE, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Perturbation split: {len(smiles_train)} train, {len(smiles_test)} test perturbations\")\n",
    "\n",
    "# Create train and test dataframes based on perturbation split\n",
    "df_train = df[df['canonical_smiles'].isin(smiles_train)].copy()\n",
    "df_test = df[df['canonical_smiles'].isin(smiles_test)].copy()\n",
    "\n",
    "print(f\"Sample split: {len(df_train)} train, {len(df_test)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Morgan fingerprints for train set...\n",
      "Generated Morgan fingerprints for train: shape (195733, 4096)\n",
      "Generating Morgan fingerprints for test set...\n",
      "Generated Morgan fingerprints for test: shape (46781, 4096)\n",
      "Applying improved scaling strategy...\n",
      "Gene expression scaled: train (195733, 983), test (46781, 983)\n",
      "Morgan fingerprints scaled: train (195733, 4096), test (46781, 4096)\n",
      "Loaded and processed control embeddings for 70 unique cells.\n"
     ]
    }
   ],
   "source": [
    "# Process train/test sets - fit preprocessing on training data only\n",
    "pert_time_mean = None\n",
    "pert_dose_mean = None\n",
    "\n",
    "for df_split, split_name in [(df_train, 'train'), (df_test, 'test')]:\n",
    "    # ignore-flag columns for missing meta-data\n",
    "    df_split['ignore_flag_pert_time'] = (df_split['pert_time'] == -666).astype(int)\n",
    "    df_split['ignore_flag_pert_dose'] = (df_split['pert_dose'] == -666).astype(int)\n",
    "\n",
    "    # replace â€“666 with column mean (computed from training set only)\n",
    "    for col in ['pert_time', 'pert_dose']:\n",
    "        if split_name == 'train':\n",
    "            mean_val = df_split.loc[df_split[col] != -666, col].mean()\n",
    "            # Store the mean for use with val/test sets\n",
    "            if col == 'pert_time':\n",
    "                pert_time_mean = mean_val\n",
    "            else:\n",
    "                pert_dose_mean = mean_val\n",
    "        else:\n",
    "            # Use training set means for test set\n",
    "            mean_val = pert_time_mean if col == 'pert_time' else pert_dose_mean\n",
    "        \n",
    "        df_split[col] = df_split[col].replace(-666, mean_val)\n",
    "\n",
    "# Function to process data split\n",
    "def process_data_split(df_split, split_name):\n",
    "    # Getting X (gene expression features)\n",
    "    numeric_cols   = df_split.select_dtypes(include=[np.number]).columns\n",
    "    drop_cols      = ['pert_dose', 'pert_dose_unit', 'pert_time',\n",
    "                      'distil_cc_q75', 'pct_self_rank_q25']\n",
    "    feature_cols   = [c for c in numeric_cols if c not in drop_cols]\n",
    "    X_raw          = df_split[feature_cols].values\n",
    "\n",
    "    # Generate Morgan fingerprints\n",
    "    print(f\"Generating Morgan fingerprints for {split_name} set...\")\n",
    "    morgan_fps = []\n",
    "    for smiles in df_split['canonical_smiles']:\n",
    "        fp = smiles_to_morgan_fp(smiles)\n",
    "        morgan_fps.append(fp)\n",
    "\n",
    "    morgan_fps = np.array(morgan_fps)\n",
    "    print(f\"Generated Morgan fingerprints for {split_name}: shape {morgan_fps.shape}\")\n",
    "\n",
    "    # Keep other context features\n",
    "    pert_unit_dummies  = pd.get_dummies(df_split['pert_dose_unit'], drop_first=True)\n",
    "\n",
    "    pert_time   = df_split['pert_time'  ].to_numpy().reshape(-1, 1)\n",
    "    pert_dose   = df_split['pert_dose'  ].to_numpy().reshape(-1, 1)\n",
    "    ignore_time = df_split['ignore_flag_pert_time'].to_numpy().reshape(-1, 1)\n",
    "    ignore_dose = df_split['ignore_flag_pert_dose'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    return X_raw, morgan_fps, pert_unit_dummies, pert_time, pert_dose, ignore_time, ignore_dose\n",
    "\n",
    "# Process both splits\n",
    "X_raw_train, morgan_fps_train, pert_unit_dummies_train, pert_time_train, pert_dose_train, ignore_time_train, ignore_dose_train = process_data_split(df_train, 'train')\n",
    "X_raw_test, morgan_fps_test, pert_unit_dummies_test, pert_time_test, pert_dose_test, ignore_time_test, ignore_dose_test = process_data_split(df_test, 'test')\n",
    "\n",
    "print(\"Applying improved scaling strategy...\")\n",
    "\n",
    "# scaling\n",
    "scaler_genes = StandardScaler()\n",
    "X_train_scaled = scaler_genes.fit_transform(X_raw_train)\n",
    "X_test_scaled = scaler_genes.transform(X_raw_test)\n",
    "print(f\"Gene expression scaled: train {X_train_scaled.shape}, test {X_test_scaled.shape}\")\n",
    "\n",
    "scaler_morgan = StandardScaler()\n",
    "morgan_train_scaled = morgan_fps_train.astype(float)\n",
    "morgan_test_scaled = morgan_fps_test.astype(float)\n",
    "print(f\"Morgan fingerprints scaled: train {morgan_train_scaled.shape}, test {morgan_test_scaled.shape}\")\n",
    "\n",
    "# Load and process control data\n",
    "ctrls_df = pd.read_csv(PATH_CTLS, index_col=0)          # index = cell_id\n",
    "unique_cells_train = np.sort(df_train['cell_id'].unique())\n",
    "unique_cells_test = np.sort(df_test['cell_id'].unique())\n",
    "unique_cells_all = np.sort(np.union1d(unique_cells_train, unique_cells_test))\n",
    "\n",
    "ctrls_df = ctrls_df.loc[ctrls_df.index.intersection(unique_cells_all)]\n",
    "\n",
    "# Standardize controls and do PCA\n",
    "scaler_ctrls = StandardScaler()\n",
    "ctrls_scaled = scaler_ctrls.fit_transform(ctrls_df.values)\n",
    "\n",
    "n_cells = ctrls_scaled.shape[0]\n",
    "n_ctrl_pcs = min(50, n_cells)\n",
    "\n",
    "pca_ctrls = PCA(n_components=n_ctrl_pcs, random_state=RANDOM_STATE)\n",
    "ctrls_pcs = pca_ctrls.fit_transform(ctrls_scaled)        # shape (n_cells, n_ctrl_pcs)\n",
    "\n",
    "# Build mapping from cell_id â†’ compressed control vector\n",
    "cell2vec = dict(zip(ctrls_df.index, ctrls_pcs))\n",
    "\n",
    "if not cell2vec:\n",
    "    raise ValueError(\n",
    "        \"No common cell IDs found between lincs1000.csv and embeddings/ctrls.csv. \"\n",
    "        \"Cannot proceed. Please check your data files.\"\n",
    "    )\n",
    "\n",
    "print(f\"Loaded and processed control embeddings for {len(cell2vec)} unique cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Build Context Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building context matrices with improved scaling...\n",
      "Fitted context scaler on (195733, 52) continuous context features\n",
      "Context matrix:   train (195733, 4150)   test (46781, 4150)\n",
      "Feature matrix:   train (195733, 983)   test (46781, 983)\n",
      "Applying PCA with improved scaling...\n",
      "Normalized PCA features: train (195733, 50)   test (46781, 50)\n"
     ]
    }
   ],
   "source": [
    "def build_context_matrix_improved(df_split, morgan_fps_scaled, pert_time, pert_dose, \n",
    "                                 ignore_time, ignore_dose, split_name, scaler_context=None, is_train=False):\n",
    "    \"\"\"\n",
    "    Build context matrix with globally consistent scaling\n",
    "    \"\"\"\n",
    "    cell_ids = df_split['cell_id'].to_numpy()\n",
    "    unique_cells_split = np.sort(df_split['cell_id'].unique())\n",
    "    \n",
    "    all_continuous_context = []\n",
    "    valid_cells = []\n",
    "    \n",
    "    for cell_id in unique_cells_split:\n",
    "        if cell_id not in cell2vec:\n",
    "            print(f\"Warning: Cell {cell_id} not found in control embeddings, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        mask = cell_ids == cell_id\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        valid_cells.append(cell_id)\n",
    "        \n",
    "        # Build continuous context matrix (cell embeddings + time + dose)\n",
    "        C_continuous = np.hstack([\n",
    "            np.tile(cell2vec[cell_id], (mask.sum(), 1)),  # Cell embeddings\n",
    "            pert_time[mask],                              # Perturbation time\n",
    "            pert_dose[mask],                              # Perturbation dose\n",
    "        ])\n",
    "        all_continuous_context.append(C_continuous)\n",
    "    \n",
    "    # Fit scaler on all continuous context (training data only)\n",
    "    if is_train:\n",
    "        all_continuous_combined = np.vstack(all_continuous_context)\n",
    "        scaler_context = StandardScaler()\n",
    "        scaler_context.fit(all_continuous_combined)\n",
    "        print(f\"Fitted context scaler on {all_continuous_combined.shape} continuous context features\")\n",
    "    \n",
    "    if scaler_context is None:\n",
    "        raise ValueError(\"scaler_context must be provided for non-training data\")\n",
    "    \n",
    "    X_lst, C_lst, cell_lst = [], [], []\n",
    "    \n",
    "    for i, cell_id in enumerate(valid_cells):\n",
    "        mask = cell_ids == cell_id\n",
    "        X_cell = X_train_scaled[mask] if split_name == 'train' else X_test_scaled[mask]\n",
    "        \n",
    "        # Scale continuous context consistently\n",
    "        C_continuous_scaled = scaler_context.transform(all_continuous_context[i])\n",
    "        \n",
    "        n_samples = mask.sum()\n",
    "        \n",
    "        # Combine all context features\n",
    "        C_cell = np.hstack([\n",
    "            C_continuous_scaled,                    # Scaled continuous features\n",
    "            morgan_fps_scaled[mask],               # Pre-scaled molecular features  \n",
    "            ignore_time[mask],                     # Binary flags (unscaled)\n",
    "            ignore_dose[mask],\n",
    "        ])\n",
    "\n",
    "        X_lst.append(X_cell)\n",
    "        C_lst.append(C_cell)\n",
    "        cell_lst.append(cell_ids[mask])\n",
    "\n",
    "    if not X_lst:\n",
    "        raise RuntimeError(f\"No data collected for {split_name} set.\")\n",
    "    \n",
    "    X_final = np.vstack(X_lst)\n",
    "    C_final = np.vstack(C_lst)\n",
    "    cell_ids_final = np.concatenate(cell_lst)\n",
    "    \n",
    "    return X_final, C_final, cell_ids_final, scaler_context\n",
    "\n",
    "# Build context matrices for both splits with improved scaling\n",
    "print(\"Building context matrices with improved scaling...\")\n",
    "\n",
    "X_train, C_train, cell_ids_train, scaler_context = build_context_matrix_improved(\n",
    "    df_train, morgan_train_scaled, pert_time_train, pert_dose_train,\n",
    "    ignore_time_train, ignore_dose_train, 'train', is_train=True\n",
    ")\n",
    "\n",
    "X_test, C_test, cell_ids_test, _ = build_context_matrix_improved(\n",
    "    df_test, morgan_test_scaled, pert_time_test, pert_dose_test,\n",
    "    ignore_time_test, ignore_dose_test, 'test', scaler_context=scaler_context\n",
    ")\n",
    "\n",
    "print(f'Context matrix:   train {C_train.shape}   test {C_test.shape}')\n",
    "print(f'Feature matrix:   train {X_train.shape}   test {X_test.shape}')\n",
    "\n",
    "# IMPROVED PCA WITH BETTER SCALING\n",
    "print(\"Applying PCA with improved scaling...\")\n",
    "\n",
    "# PCA on features (fit on training data only)\n",
    "pca_data = PCA(n_components=N_DATA_PCS, random_state=RANDOM_STATE)\n",
    "X_train_pca = pca_data.fit_transform(X_train)\n",
    "X_test_pca = pca_data.transform(X_test)\n",
    "\n",
    "# Improved scaling in PCA space\n",
    "pca_scaler = StandardScaler()\n",
    "X_train_norm = pca_scaler.fit_transform(X_train_pca)\n",
    "X_test_norm = pca_scaler.transform(X_test_pca)\n",
    "\n",
    "print(f'Normalized PCA features: train {X_train_norm.shape}   test {X_test_norm.shape}')\n",
    "\n",
    "# Set useful variables\n",
    "train_group_ids = cell_ids_train\n",
    "test_group_ids = cell_ids_test\n",
    "X_train = X_train_norm\n",
    "X_test = X_test_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Fit Population Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.9800000000000005\n",
      "Test MSE: 0.9598707173826789\n"
     ]
    }
   ],
   "source": [
    "from contextualized.baselines.networks import CorrelationNetwork\n",
    "pop_model = CorrelationNetwork()\n",
    "pop_model.fit(X_train)\n",
    "print(f\"Train MSE: {pop_model.measure_mses(X_train).mean()}\")\n",
    "print(f\"Test MSE: {pop_model.measure_mses(X_test).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Fit Grouped Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Train MSE: 0.5788541778716761\n"
     ]
    }
   ],
   "source": [
    "from contextualized.baselines.networks import GroupedNetworks\n",
    "grouped_model = GroupedNetworks(CorrelationNetwork)\n",
    "grouped_model.fit(X_train, train_group_ids)\n",
    "print(f\"Grouped Train MSE: {grouped_model.measure_mses(X_train, train_group_ids).mean()}\")\n",
    "print(f\"Grouped Test MSE: {grouped_model.measure_mses(X_test, test_group_ids).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Fit Contextualized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /mmfs1/home/jiaqiw18/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjiaqiw\u001b[0m (\u001b[33mcontextualized\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ContextualizedCorrelation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlogin(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m443a02df5197cc2c2c579a9ff78179ff45e47824\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Add your WandB API key here\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m contextualized_model \u001b[38;5;241m=\u001b[39m \u001b[43mContextualizedCorrelation\u001b[49m(\n\u001b[1;32m      5\u001b[0m     context_dim\u001b[38;5;241m=\u001b[39mC_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      6\u001b[0m     x_dim\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      7\u001b[0m     encoder_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     num_archetypes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Random val split\u001b[39;00m\n\u001b[1;32m     11\u001b[0m C_val \u001b[38;5;241m=\u001b[39m train_test_split(C_train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ContextualizedCorrelation' is not defined"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key='443a02df5197cc2c2c579a9ff78179ff45e47824')  # Add your WandB API key here\n",
    "\n",
    "contextualized_model = ContextualizedCorrelation(\n",
    "    context_dim=C_train.shape[1],\n",
    "    x_dim=X_train.shape[1],\n",
    "    encoder_type='mlp',\n",
    "    num_archetypes=30,\n",
    ")\n",
    "# Random val split\n",
    "C_val = train_test_split(C_train, test_size=0.2, random_state=RANDOM_STATE)[0]\n",
    "X_val = train_test_split(X_train, test_size=0.2, random_state=RANDOM_STATE)[0]\n",
    "datamodule = CorrelationDataModule(\n",
    "    C_train=C_train,\n",
    "    X_train=X_train,\n",
    "    C_val=C_val,\n",
    "    X_val=X_val,\n",
    "    C_test=C_test,\n",
    "    X_test=X_test,\n",
    "    C_predict=np.concatenate((C_train, C_test), axis=0),\n",
    "    X_predict=np.concatenate((X_train, X_test), axis=0),\n",
    "    batch_size=32,\n",
    ")\n",
    "checkpoint_callback = pl.pytorch.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    filename='best_model',\n",
    ")\n",
    "logger = pl.pytorch.loggers.WandbLogger(\n",
    "    project='contextpert',\n",
    "    name='unseen_perturbations',\n",
    "    log_model=True,\n",
    "    save_dir='logs/',\n",
    ")\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator='auto',\n",
    "    devices='auto',\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "trainer.fit(contextualized_model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Testing model on training data...\")\n",
    "trainer.test(contextualized_model, datamodule.train_dataloader())\n",
    "print(f\"Testing model on test data...\")\n",
    "trainer.test(contextualized_model, datamodule.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Predict Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary to save predictions from multiple devices in parallel\n",
    "from contextualized.callbacks import PredictionWriter\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(checkpoint_callback.best_model_path).parent / 'predictions'\n",
    "writer_callback = PredictionWriter(\n",
    "    output_dir=output_dir,\n",
    "    write_interval='batch',\n",
    ")\n",
    "trainer = Trainer(\n",
    "    accelerator='auto',\n",
    "    devices='auto',\n",
    "    callbacks=[checkpoint_callback, writer_callback],\n",
    ")\n",
    "_ = trainer.predict(contextualized_model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile distributed predictions and put into order\n",
    "\"\"\"\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "# Convert context to hashable type for lookup\n",
    "C_train_hashable = [tuple(row) for row in C_train]\n",
    "C_test_hashable = [tuple(row) for row in C_test]\n",
    "\n",
    "# Gather preds and move to CPU\n",
    "all_correlations = {}\n",
    "all_betas = {}\n",
    "all_mus = {}\n",
    "pred_files = glob.glob(str(output_dir / 'predictions_*.pt'))\n",
    "for file in pred_files:\n",
    "    preds = torch.load(file)\n",
    "    for context, correlation, beta, mu in zip(preds['contexts'], preds['correlations'], preds['betas'], preds['mus']):\n",
    "        context_tuple = tuple(context.tolist())\n",
    "        all_correlations[context_tuple] = correlation.cpu().numpy()\n",
    "        all_betas[context_tuple] = beta.cpu().numpy()\n",
    "        all_mus[context_tuple] = mu.cpu().numpy()\n",
    "\n",
    "# Remake preds in order of C_train and C_test\n",
    "correlations_train = np.array([all_correlations[c] for c in C_train_hashable])\n",
    "correlations_test = np.array([all_correlations[c] for c in C_test_hashable])\n",
    "betas_train = np.array([all_betas[c] for c in C_train_hashable])\n",
    "betas_test = np.array([all_betas[c] for c in C_test_hashable])\n",
    "mus_train = np.array([all_mus[c] for c in C_train_hashable])\n",
    "mus_test = np.array([all_mus[c] for c in C_test_hashable])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d624e98-941e-4ac5-bed1-3d6e4608d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile distributed predictions and put into order\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "C_train = np.round(C_train.astype(np.float32), 6)\n",
    "C_test  = np.round(C_test.astype(np.float32), 6)\n",
    "\n",
    "# Convert context to hashable type for lookup\n",
    "C_train_hashable = [tuple(row) for row in C_train]\n",
    "C_test_hashable = [tuple(row) for row in C_test]\n",
    "\n",
    "# Gather preds and move to CPU\n",
    "all_correlations = {}\n",
    "all_betas = {}\n",
    "all_mus = {}\n",
    "pred_files = glob.glob(str(output_dir / 'predictions_*.pt'))\n",
    "for file in pred_files:\n",
    "    preds = torch.load(file)\n",
    "    for context, correlation, beta, mu in zip(preds['contexts'], preds['correlations'], preds['betas'], preds['mus']):\n",
    "        context_tuple = tuple(np.round(context.cpu().numpy(), 6))\n",
    "        # context_tuple = tuple(context.tolist())\n",
    "        all_correlations[context_tuple] = correlation.cpu().numpy()\n",
    "        all_betas[context_tuple] = beta.cpu().numpy()\n",
    "        all_mus[context_tuple] = mu.cpu().numpy()\n",
    "\n",
    "# Remake preds in order of C_train and C_test\n",
    "correlations_train = np.array([all_correlations[c] for c in C_train_hashable])\n",
    "correlations_test = np.array([all_correlations[c] for c in C_test_hashable])\n",
    "betas_train = np.array([all_betas[c] for c in C_train_hashable])\n",
    "betas_test = np.array([all_betas[c] for c in C_test_hashable])\n",
    "mus_train = np.array([all_mus[c] for c in C_train_hashable])\n",
    "mus_test = np.array([all_mus[c] for c in C_test_hashable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get individual MSEs by sample\n",
    "# Sanity check: These should closely match the trainer.test() outputs from earlier\n",
    "def measure_mses(betas, mus, X):\n",
    "    mses = np.zeros(len(X))\n",
    "    for i in range(len(X)):\n",
    "        sample_mse = 0\n",
    "        for j in range(X.shape[-1]):\n",
    "            for k in range(X.shape[-1]):\n",
    "                residual = X[i, j] - betas[i, j, k] * X[i, k] - mus[i, j, k]\n",
    "                sample_mse += residual**2 / (X.shape[-1] ** 2)\n",
    "        mses += sample_mse / len(X)\n",
    "    return mses\n",
    "\n",
    "mse_train = measure_mses(betas_train, mus_train, X_train)\n",
    "mse_test = measure_mses(betas_test, mus_test, X_test)\n",
    "print(f\"Train MSEs: {mse_train.mean()}\")\n",
    "print(f\"Test MSEs: {mse_test.mean()}\")\n",
    "\n",
    "# Per-cell performance breakdown\n",
    "print(f\"\\nPer-cell performance breakdown:\")\n",
    "print(\"Cell ID          Train MSE    Test MSE     Train N  Test N\")\n",
    "print(\"â”€\" * 60)\n",
    "\n",
    "all_unique_cells = np.union1d(cell_ids_train, cell_ids_test)\n",
    "\n",
    "for cell_id in sorted(all_unique_cells):\n",
    "    tr_mask = cell_ids_train == cell_id\n",
    "    te_mask = cell_ids_test == cell_id\n",
    "    \n",
    "    tr_mse = mse_train[tr_mask].mean() if tr_mask.any() else np.nan\n",
    "    te_mse = mse_test[te_mask].mean() if te_mask.any() else np.nan\n",
    "    tr_n = tr_mask.sum()\n",
    "    te_n = te_mask.sum()\n",
    "    \n",
    "    if tr_n > 0 or te_n > 0:\n",
    "        print(f'{cell_id:<15}  {tr_mse:8.4f}   {te_mse:8.4f}   {tr_n:6d}   {te_n:6d}')\n",
    "\n",
    "# Summary statistics about the perturbation split\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"PERTURBATION HOLDOUT SUMMARY:\")\n",
    "print(f\"  Total unique SMILES: {len(unique_smiles)}\")\n",
    "print(f\"  Training SMILES: {len(smiles_train)} ({len(smiles_train)/len(unique_smiles)*100:.1f}%)\")\n",
    "print(f\"  Test SMILES: {len(smiles_test)} ({len(smiles_test)/len(unique_smiles)*100:.1f}%)\")\n",
    "print(f\"  Training samples: {len(df_train)}\")\n",
    "print(f\"  Test samples: {len(df_test)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bf098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, warnings, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "OUT_DIR   = pathlib.Path(\"figs/pert_similarity_clustermaps\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "GROUP_KEY = \"canonical_smiles\"\n",
    "MAX_PERTS = 10_000\n",
    "RNG       = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbdf63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdist(mat: np.ndarray):\n",
    "    xy = mat @ mat.T\n",
    "    x2 = y2 = (mat * mat).sum(1)\n",
    "    d2 = np.add.outer(x2, y2) - 2 * xy\n",
    "    d2.flat[::len(mat)+1] = 0\n",
    "    d2[d2 < 0] = 0\n",
    "    return squareform(np.sqrt(d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df_train.select_dtypes(include=[np.number]).columns\n",
    "drop_cols    = ['pert_dose', 'pert_dose_unit', 'pert_time',\n",
    "                'distil_cc_q75', 'pct_self_rank_q25']\n",
    "gene_cols    = [c for c in numeric_cols if c not in drop_cols]\n",
    "\n",
    "expr_all   = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n",
    "expr_mat   = expr_all[gene_cols].to_numpy(dtype=np.float32)\n",
    "smiles_all = expr_all[GROUP_KEY].to_numpy()\n",
    "\n",
    "print(f\"Gene-expression raw matrix: {expr_mat.shape}\")\n",
    "\n",
    "# standardize then PCA\n",
    "scaler_expr = StandardScaler()\n",
    "expr_scaled = scaler_expr.fit_transform(expr_mat)\n",
    "\n",
    "pca_expr    = PCA(n_components=50, random_state=42)\n",
    "expr_pcs    = pca_expr.fit_transform(expr_scaled)      # (n_samples, 50)\n",
    "\n",
    "# aggregate\n",
    "expr_df = pd.DataFrame(expr_pcs)\n",
    "expr_df[GROUP_KEY] = smiles_all\n",
    "expr_repr_df = expr_df.groupby(GROUP_KEY).mean()       # (perts Ã— 50)\n",
    "\n",
    "expr_repr   = expr_repr_df.to_numpy(dtype=np.float32)\n",
    "smiles_order = expr_repr_df.index.tolist()\n",
    "\n",
    "print(f\"Expression representation: {expr_repr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f32a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "\n",
    "morgan_gen = rdFingerprintGenerator.GetMorganGenerator(radius=3, fpSize=4096)\n",
    "\n",
    "def smiles_to_fp(smiles, gen=morgan_gen):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        warnings.warn(f\"Invalid SMILES: {smiles}\")\n",
    "        return np.zeros(gen.GetOptions().fpSize, dtype=np.float32)\n",
    "    return np.asarray(gen.GetFingerprint(mol), dtype=np.float32)\n",
    "\n",
    "fp_repr = np.vstack([smiles_to_fp(sm) for sm in smiles_order])\n",
    "print(f\"Fingerprint representation: {fp_repr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71bf6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _aligned_smiles(df_split, C_split, key=GROUP_KEY):\n",
    "    blocks = []\n",
    "    for cell_id in df_split['cell_id'].unique():\n",
    "        mask = df_split['cell_id'] == cell_id\n",
    "        blocks.append(df_split.loc[mask, key].to_numpy())\n",
    "    return np.concatenate(blocks)\n",
    "\n",
    "smiles_train_aligned = _aligned_smiles(df_train, C_train)\n",
    "smiles_test_aligned  = _aligned_smiles(df_test,  C_test)\n",
    "\n",
    "assert len(smiles_train_aligned) == C_train.shape[0]\n",
    "assert len(smiles_test_aligned)  == C_test.shape[0]\n",
    "\n",
    "print(f\"Aligned SMILES arrays built: train {smiles_train_aligned.shape}, \"\n",
    "      f\"test {smiles_test_aligned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46336b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_upper(mat3d: np.ndarray):\n",
    "    p = mat3d.shape[1]\n",
    "    iu = np.triu_indices(p, k=1)\n",
    "    return mat3d[:, iu[0], iu[1]].astype(np.float32)   # (n_samples, p*(p-1)/2)\n",
    "\n",
    "# concatenate train+test, flatten\n",
    "corr_flat_all = flat_upper(np.concatenate([correlations_train,\n",
    "                                           correlations_test], axis=0))\n",
    "smiles_ctx_all = np.concatenate([smiles_train_aligned,\n",
    "                                 smiles_test_aligned])\n",
    "\n",
    "# mean per SMILES\n",
    "corr_df = pd.DataFrame(corr_flat_all)\n",
    "corr_df[GROUP_KEY] = smiles_ctx_all\n",
    "corr_df = corr_df.groupby(GROUP_KEY).mean().reindex(smiles_order)\n",
    "\n",
    "if corr_df.isna().any().any():\n",
    "    dropped = corr_df.index[corr_df.isna().all(axis=1)]\n",
    "    print(f\"[ctx_corr] dropped {len(dropped)} perturbations lacking predictions.\")\n",
    "    corr_df = corr_df.dropna(how='all')\n",
    "\n",
    "ctx_repr_corr = corr_df.to_numpy(dtype=np.float32)\n",
    "print(f\"[ctx_corr] representation shape: {ctx_repr_corr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ba59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns, matplotlib.pyplot as plt, pathlib, numpy as np\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "OUT_DIR   = pathlib.Path(\"figs/pert_similarity_clustermaps\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MAX_PERTS = 10_000\n",
    "_rng      = np.random.default_rng(42)\n",
    "\n",
    "def cdist_fast(mat: np.ndarray):\n",
    "    xy = mat @ mat.T\n",
    "    norms = (mat * mat).sum(1)\n",
    "    d2 = np.add.outer(norms, norms) - 2 * xy\n",
    "    d2.flat[::len(mat)+1] = 0\n",
    "    d2[d2 < 0] = 0\n",
    "    return squareform(np.sqrt(d2, dtype=np.float32))\n",
    "\n",
    "def make_clustermap(X: np.ndarray, label: str):\n",
    "    n = X.shape[0]\n",
    "    if MAX_PERTS and n > MAX_PERTS:\n",
    "        idx = _rng.choice(n, MAX_PERTS, replace=False)\n",
    "        X = X[idx]\n",
    "        print(f\"[{label}] sub-sampled {n} - {len(idx)} perturbations.\")\n",
    "    dist_cond = cdist_fast(X.astype(np.float32, copy=False))\n",
    "    linkage   = hierarchy.linkage(dist_cond, method=\"average\")\n",
    "    dist_sq   = squareform(dist_cond)\n",
    "\n",
    "    g = sns.clustermap(dist_sq,\n",
    "                       row_linkage=linkage,\n",
    "                       col_linkage=linkage,\n",
    "                       cmap=\"vlag\",\n",
    "                       figsize=(12, 12),\n",
    "                       xticklabels=False,\n",
    "                       yticklabels=False,\n",
    "                       cbar_kws={'label': 'Euclidean distance'})\n",
    "    g.fig.suptitle(f\"{label.upper()} - perturbation similarity\", y=1.02)\n",
    "\n",
    "    png = OUT_DIR / f\"clustermap_{label}.png\"\n",
    "    npy = OUT_DIR / f\"dist_square_{label}.npy\"\n",
    "    g.savefig(png, bbox_inches=\"tight\")\n",
    "    np.save(npy, dist_sq)\n",
    "    plt.show()\n",
    "    print(f\"[{label}] saved âžœ {png.name}  |  {npy.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_clustermap(expr_repr,     \"expr\")\n",
    "make_clustermap(fp_repr,       \"fp\")\n",
    "make_clustermap(ctx_repr_corr, \"ctx_corr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
